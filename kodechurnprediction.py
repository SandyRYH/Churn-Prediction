# -*- coding: utf-8 -*-
"""KodeChurnPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/131l3DFsgktWoD39W2eNyFFURYdhhyF1E
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LassoCV
from sklearn.feature_selection import SelectKBest, chi2

"""# Membaca dan menyiapkan data"""

# Baca dataset churn
df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')
df.head()

# Buang record yang memiliki missing value
df.dropna(inplace=True)
df.info()

"""Jumlah record tidak berubah, berarti tidak ada missing value pada dataset (sudah lengkap)"""

# Summary dari kolom bertipe numerik
df.describe()

"""# Membagi dataset menjadi fitur (X) dan target (y)"""

# Membagi dataset menjadi X (fitur) dan Y (label) untuk klasifikasi
# Data frame fitur (customerId dibuang karena tidak diperlukan)
X=df.iloc[:,1:-1]

# Data frame target Churn (label)
y=df["Churn"]
y=pd.DataFrame(y)

X.info()

y.head()
X.head()

# Konversi fitur kategorikal menjadi numerik

from sklearn.preprocessing import LabelEncoder

y=LabelEncoder().fit_transform(y)

X['gender']=LabelEncoder().fit_transform(X['gender'])
X['Partner']=LabelEncoder().fit_transform(X['Partner'])
X['Dependents']=LabelEncoder().fit_transform(X['Dependents'])
X['PhoneService']=LabelEncoder().fit_transform(X['PhoneService'])
X['MultipleLines']=LabelEncoder().fit_transform(X['MultipleLines'])
X['InternetService']=LabelEncoder().fit_transform(X['InternetService'])
X['OnlineSecurity']=LabelEncoder().fit_transform(X['OnlineSecurity'])
X['OnlineBackup']=LabelEncoder().fit_transform(X['OnlineBackup'])
X['DeviceProtection']=LabelEncoder().fit_transform(X['DeviceProtection'])
X['StreamingTV']=LabelEncoder().fit_transform(X['StreamingTV'])
X['StreamingMovies']=LabelEncoder().fit_transform(X['StreamingMovies'])
X['Contract']=LabelEncoder().fit_transform(X['Contract'])
X['PaperlessBilling']=LabelEncoder().fit_transform(X['PaperlessBilling'])
X['PaymentMethod']=LabelEncoder().fit_transform(X['PaymentMethod'])
X['TechSupport']=LabelEncoder().fit_transform(X['TechSupport'])
X['TechSupport']=LabelEncoder().fit_transform(X['TechSupport'])

X.info()

X.head()

y

# Handle value TotalCharges yang tidak bisa dikonversi menjadi numerik
# Ubah menjadi NA, lalu ganti dengan nilai mediannya
X['TotalCharges'] = pd.to_numeric(X['TotalCharges'],errors='coerce')
median = X['TotalCharges'].median()
X['TotalCharges'].fillna(median,inplace=True)

X.head()
X.info()

"""# Seleksi Fitur / Feature Selection dengan Lasso"""

reg = LassoCV()
reg.fit(X, y)
print("Alpha terbaik menggunakan fungsi built-in LassoCV: %f" % reg.alpha_)
print("Score terbaik menggunakan fungsi built-in LassoCV: %f" %reg.score(X,y))
coef = pd.Series(reg.coef_, index = X.columns)
print("Lasso memilih " + str(sum(coef != 0)) + " fitur dan mengeliminasi " + str(sum(coef == 0)) + " fitur lainnya")

# Create a DataFrame to store the feature importance
feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': reg.coef_})

# Sort the features by the index in ascending order
feature_importance = feature_importance.sort_index()

# Display the feature importance table
print(feature_importance)

# Plotting nilai importance tiap fitur
imp_coef = coef.sort_values()
import matplotlib
matplotlib.rcParams['figure.figsize'] = (6,6)
imp_coef.plot(kind = "barh")
plt.title("Feature Importance dari model Lasso")
plt.show()

"""Maka, jika kita menggunakan Lasso, terpilih 3 fitur yaitu Monthly Charges, TotalCharges, dan tenure

# Normaliasi Data Fitur
"""

# Normalisasi dengan MinMaxScaler
from sklearn.preprocessing import MinMaxScaler
names=X.columns
indexes=X.index
X=MinMaxScaler().fit_transform(X)
X=pd.DataFrame(X,columns=names,index=indexes)
X.head()

"""# Seleksi Fitur / Feature Selection dengan SelectKBest (chi2)"""

kmodel=SelectKBest(score_func=chi2,k=8)
x_clf_new=kmodel.fit_transform(X,y)
mask=kmodel.get_support()
important=X.columns[mask]
print(important,len(important))

from sklearn.feature_selection import SelectKBest, chi2
import pandas as pd

# Perform feature selection
kmodel = SelectKBest(score_func=chi2, k=8)
x_clf_new = kmodel.fit_transform(X, y)

# Get the scores and corresponding feature names
scores = kmodel.scores_
feature_names = X.columns

# Create a DataFrame to display the scores
scores_df = pd.DataFrame({'Feature': feature_names, 'Score': scores})

# Sort the DataFrame by score in descending order
scores_df = scores_df.sort_values(by='Score', ascending=False)

# Display the top features with their scores
print(scores_df.head(19))

"""Maka, jika kita menggunakan SelectKBest, terpilih 8 fitur yaitu SeniorCitizen, Dependents, tenure, OnlineSecurity, OnlineBackup, TechSupport, Contract, dan PaperlessBilling

# Membagi Data menjadi 3 set: train, test dan validation
"""

from sklearn.model_selection import train_test_split
X_train,X_rem,y_train,y_rem = train_test_split(X, y, test_size=0.20, random_state=42)
X_val,X_test,y_val,y_test = train_test_split(X_rem, y_rem, test_size=0.50, random_state=42)

"""# Implementasi Logistic Regression

1. Tanpa Feature Selection
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
log = LogisticRegression()
log.fit(X_train,y_train)

# Akurasi Validation
acc = accuracy_score(log.predict(X_val),y_val)
f'Validation accuracy: {round(acc*100,2)}%'

# Get the coefficients and intercept
coefficients = log.coef_
intercept = log.intercept_

# Display the regression function
print("Regression function:")
print("Intercept:", intercept)
for i, feature in enumerate(X.columns):
    print(f"Coefficient for {feature}: {coefficients[0][i]}")

# Akurasi Testing
acc = accuracy_score(log.predict(X_test),y_test)
f'Testing accuracy: {round(acc*100,2)}%'

"""2. Dengan fitur hasil seleksi Lasso"""

log.fit(X_train[["MonthlyCharges","TotalCharges","tenure"]],y_train)

# Akurasi Validation
acc = accuracy_score(log.predict(X_val[["MonthlyCharges","TotalCharges","tenure"]]),y_val)
f'Validation accuracy: {round(acc*100,2)}%'

# Akurasi Testing
acc = accuracy_score(log.predict(X_test[["MonthlyCharges","TotalCharges","tenure"]]),y_test)
f'Testing accuracy: {round(acc*100,2)}%'

"""3. Dengan fitur hasil seleksi KBest"""

log.fit(X_train[['SeniorCitizen', 'Dependents', 'tenure', 'OnlineSecurity','OnlineBackup', 'TechSupport', 'Contract', 'PaperlessBilling']],y_train)

# Akurasi Validation
acc = accuracy_score(log.predict(X_val[['SeniorCitizen', 'Dependents', 'tenure', 'OnlineSecurity','OnlineBackup', 'TechSupport', 'Contract', 'PaperlessBilling']]),y_val)
f'Validation accuracy: {round(acc*100,2)}%'

# Akurasi Testing
acc = accuracy_score(log.predict(X_test[['SeniorCitizen', 'Dependents', 'tenure', 'OnlineSecurity','OnlineBackup', 'TechSupport', 'Contract', 'PaperlessBilling']]),y_test)
f'Testing accuracy: {round(acc*100,2)}%'

"""# Implementasi Decision Tree


"""

from sklearn.tree import DecisionTreeClassifier, export_graphviz, ExtraTreeClassifier, plot_tree
dt = DecisionTreeClassifier(criterion='entropy',max_depth=3)
dt = dt.fit(X_train,y_train)

# Akurasi Training
y_pred = dt.predict(X_train)
acc = accuracy_score(y_train,y_pred)
f'Training accuracy: {round(acc*100,2)}%'

# Akurasi Validation
y_pred = dt.predict(X_val)
acc = accuracy_score(y_val,y_pred)
f'Validation accuracy: {round(acc*100,2)}%'

# Akurasi Testing
y_pred = dt.predict(X_test)
acc = accuracy_score(y_test,y_pred)
f'Testing accuracy: {round(acc*100,2)}%'

plt.figure(figsize=(10, 6), dpi=300)  # Adjust the figsize and dpi values as per your preference
plot_tree(dt, filled=True, rounded=True, feature_names=X.columns)
plt.show()

"""# Implementasi Random Forest"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100, random_state=0)
rf.fit(X_train,y_train)

# Akurasi Training
y_pred = dt.predict(X_train)
acc = accuracy_score(y_train,y_pred)
f'Training accuracy: {round(acc*100,2)}%'

# Akurasi Validation
y_pred = dt.predict(X_train)
acc = accuracy_score(y_train,y_pred)
f'Training accuracy: {round(acc*100,2)}%'

# Akurasi Testing
y_pred = rf.predict(X_test)
acc = accuracy_score(y_test,y_pred)
f'Testing accuracy: {round(acc*100,2)}%'

# Get feature importances from the random forest model
importances = rf.feature_importances_

# Get the names of the features
feature_names = X.columns

# Create a DataFrame to store the feature importances
feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': importances})

# Sort the features by importance
feature_importance = feature_importance.sort_values(by='Importance', ascending=False)

# Plotting the feature importances
plt.figure(figsize=(8, 6))
plt.barh(feature_importance['Feature'], feature_importance['Importance'])
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance - Random Forest')
plt.show()

"""# Kesimpulan

Algoritme Logistic Regression tanpa seleksi fitur menghasilkan model yang paling akurat dengan nilai akurasi testing 82.41% dibandingkan algoritme Decision Tree (78.87%) dan Random Forest (79.72%).

Penggunaan feature selection baik dengan Lasso maupun KBest menurunkan nilai akurasi menjadi 80.57% (Lasso) dan 80.14% (KBest). Namun, dengan memilih fitur-fitur yang penting saja, performa model saat training maupun saat inferensi dapat ditingkatkan.
"""